{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('3.8.6')",
   "metadata": {
    "interpreter": {
     "hash": "9f3d1612ed1d757156d676f5af0447f71a555aea633ff78518b4612169104897"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.google.com\"\n",
    "keyword = \"スクレイピング\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "search = driver.find_element_by_name(\"q\")\n",
    "search.send_keys(keyword)\n",
    "search.submit()\n",
    "\n",
    "time.sleep(3)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: データ収集を大幅に効率化する「スクレイピング」とは ... \n2: ウェブスクレイピング - Wikipedia \n3: スクレイピングとは？活用事例・メリット・デメリット・導入 ... \n4: スクレイピングツール30選｜初心者でもWebデータを抽出 ... \n5: スクレイピング - Qiita \n6: 疑問に答えます！なぜWebスクレイピングを学ぶのか？ - Qiita \n7: スクレイピングとAPIの違い | NTT Communications Developer ... \n8: 【初心者向け】PythonでWebスクレイピングをしてみよう ... \n9: PythonでHTMLを解析してデータ収集してみる ... \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.google.com\"\n",
    "keyword = \"スクレイピング\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "search = driver.find_element_by_name(\"q\")\n",
    "search.send_keys(keyword)\n",
    "search.submit()\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "results = soup.find_all(\"h3\", attrs={\"class\": \"LC20lb\"})\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(\"%d: %s \" % (i + 1, result.get_text()))\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "# Twitterにログイン\n",
    "\n",
    "# 以下で.env内の環境変数をロード: https://nerimplo.hatenablog.com/entry/2018/11/21/170000\n",
    "# jupyter以外での設定: https://qiita.com/hedgehoCrow/items/2fd56ebea463e7fc0f5b\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "username = os.environ['USER_NAME']\n",
    "password = os.environ['PASSWORD']\n",
    "text = \"@noguchi_nognog2 うんち！\"\n",
    "\n",
    "url = \"https://twitter.com/login\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "username_form = driver.find_element_by_name(\"session[username_or_email]\")\n",
    "password_form = driver.find_element_by_name(\"session[password]\")\n",
    "username_form.send_keys(username)\n",
    "password_form.send_keys(password)\n",
    "password_form.submit()\n",
    "time.sleep(3)\n",
    "\n",
    "tweet_area = driver.find_element_by_class_name(\"public-DraftEditor-content\")\n",
    "tweet_area.send_keys(text)\n",
    "\n",
    "time.sleep(15)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "set\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "url = \"https://srad.jp/\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "width = driver.execute_script(\"return document.body.scrollWidth\")\n",
    "height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "driver.set_window_size(width, height)\n",
    "\n",
    "print(\"set\")\n",
    "\n",
    "time.sleep(3)\n",
    "driver.save_screenshot(\"screenshot.png\")\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "set\n"
     ]
    }
   ],
   "source": [
    "# ヘッドレスモードで実行\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "\n",
    "url = \"https://srad.jp/\"\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)\n",
    "\n",
    "width = driver.execute_script(\"return document.body.scrollWidth\")\n",
    "height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "driver.set_window_size(width, height)\n",
    "\n",
    "print(\"set\")\n",
    "\n",
    "time.sleep(3)\n",
    "driver.save_screenshot(\"screenshot.png\")\n",
    "# ページ全体が撮影される\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: データ収集を大幅に効率化する「スクレイピング」とは ... \n2: ウェブスクレイピング - Wikipedia \n3: スクレイピングとは？活用事例・メリット・デメリット・導入 ... \n4: スクレイピングツール30選｜初心者でもWebデータを抽出 ... \n5: スクレイピング - Qiita \n6: 疑問に答えます！なぜWebスクレイピングを学ぶのか？ - Qiita \n7: スクレイピングとAPIの違い | NTT Communications Developer ... \n8: 【初心者向け】PythonでWebスクレイピングをしてみよう ... \n9: PythonでHTMLを解析してデータ収集してみる ... \n10: スクレイピングとは？活用方法や注意点、確認すべきことを ... \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "\n",
    "url = \"https://www.google.com\"\n",
    "keyword = \"スクレイピング\"\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "search = driver.find_element_by_name(\"q\")\n",
    "search.send_keys(keyword)\n",
    "search.submit()\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "results = soup.find_all(\"h3\", attrs={\"class\": \"LC20lb\"})\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(\"%d: %s \" % (i + 1, result.get_text()))\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\nも\t助詞,係助詞,*,*,*,*,も,モ,モ\nもも\t名詞,一般,*,*,*,*,もも,モモ,モモ\nも\t助詞,係助詞,*,*,*,*,も,モ,モ\nもも\t名詞,一般,*,*,*,*,もも,モモ,モモ\nの\t助詞,連体化,*,*,*,*,の,ノ,ノ\nうち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\nEOS\n\n"
     ]
    }
   ],
   "source": [
    "# P.98 \n",
    "import MeCab\n",
    "\n",
    "m = MeCab.Tagger()\n",
    "print(m.parse(\"すもももももももものうち\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Google\t名詞,固有名詞,組織,*,*,*,*\nPixel\t名詞,一般,*,*,*,*,*\n（\t記号,括弧開,*,*,*,*,（,（,（\nグーグル\t名詞,固有名詞,一般,*,*,*,*\nピクセル\t名詞,一般,*,*,*,*,ピクセル,ピクセル,ピクセル\n）\t記号,括弧閉,*,*,*,*,）,）,）\nは\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n、\t記号,読点,*,*,*,*,、,、,、\nGoogle\t名詞,固有名詞,組織,*,*,*,*\nによって\t助詞,格助詞,連語,*,*,*,によって,ニヨッテ,ニヨッテ\n開発\t名詞,サ変接続,*,*,*,*,開発,カイハツ,カイハツ\nさ\t動詞,自立,*,*,サ変・スル,未然レル接続,する,サ,サ\nれ\t動詞,接尾,*,*,一段,連用形,れる,レ,レ\nた\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\nモバイル\t名詞,一般,*,*,*,*,モバイル,モバイル,モバイル\n端末\t名詞,一般,*,*,*,*,端末,タンマツ,タンマツ\nや\t助詞,並立助詞,*,*,*,*,や,ヤ,ヤ\nノート\t名詞,一般,*,*,*,*,ノート,ノート,ノート\nパソコン\t名詞,一般,*,*,*,*,パソコン,パソコン,パソコン\n製品\t名詞,一般,*,*,*,*,製品,セイヒン,セイヒン\nの\t助詞,連体化,*,*,*,*,の,ノ,ノ\nブランド\t名詞,一般,*,*,*,*,ブランド,ブランド,ブランド\nで\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\nある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n[\t名詞,サ変接続,*,*,*,*,*\n1\t名詞,数,*,*,*,*,*\n][\t名詞,サ変接続,*,*,*,*,*\n2\t名詞,数,*,*,*,*,*\n]。\t名詞,サ変接続,*,*,*,*,*\nGoogle\t名詞,一般,*,*,*,*,*\nNexus\t名詞,一般,*,*,*,*,*\nと\t助詞,格助詞,一般,*,*,*,と,ト,ト\nは\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n異なる\t動詞,自立,*,*,五段・ラ行,基本形,異なる,コトナル,コトナル\n。\t記号,句点,*,*,*,*,。,。,。\nEOS\n\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ja.wikipedia.org/wiki/Google_Pixel\"\n",
    "m = MeCab.Tagger()\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "text = soup.find(\"p\").get_text()\n",
    "\n",
    "parse_text = m.parse(text)\n",
    "print(parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Google Pixel （ グーグル ピクセル ） は 、 Google によって 開発 さ れ た モバイル 端末 や ノート パソコン 製品 の ブランド で ある [ 1 ][ 2 ]。 Google Nexus と は 異なる 。 \n\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ja.wikipedia.org/wiki/Google_Pixel\"\n",
    "m = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "text = soup.find(\"p\").get_text()\n",
    "\n",
    "parse_text = m.parse(text)\n",
    "print(parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "domain = \"https://ja.wikipedia.org/wiki/\"\n",
    "\n",
    "names = [\"森鴎外\", \"夏目漱石\", \"島崎藤村\", \"与謝野晶子\",\n",
    "         \"坪内逍遥\", \"石川啄木\", \"谷崎潤一郎\", \"芥川龍之介\",\n",
    "         \"萩原朔太郎\", \"川端康成\", \"志賀直哉\", \"中原中也\",\n",
    "         \"太宰治\", \"大岡昇平\", \"三島由紀夫\"]\n",
    "\n",
    "m = MeCab.Tagger(\"-Owakati\")\n",
    "corpus = []\n",
    "\n",
    "for name in names:\n",
    "    with requests.get(domain + name) as response:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        p_text = soup.find_all(\"p\")\n",
    "        for p in p_text:\n",
    "            corpus.append(m.parse(p.text).strip())\n",
    "\n",
    "with open(\"data.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "時代 0.99\n世界 0.98\n小説 0.98\n文章 0.98\n題材 0.98\n物語 0.98\n告白 0.98\n日本 0.98\n愛 0.98\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "file = open(\"data.txt\")\n",
    "corpus = file.read().splitlines()\n",
    "\n",
    "corpus = [sentence.split() for sentence in corpus]\n",
    "model = word2vec.Word2Vec(corpus, size=200, min_count=20, window=10)\n",
    "\n",
    "# \"文学\" ([\"文学\", \"小説\"]とかの場合はそれらの合成ベクトルとなる)のベクトルに対して、類似度が高い順にtopn個のワードを取り出す\n",
    "similar_words = model.wv.most_similar(positive=[\"文学\"], topn=9)\n",
    "print(*[\" \".join([v, str(\"{:.2f}\".format(s))]) for v, s in similar_words], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelのセーブとロード\n",
    "model.save(\"W2V.model\")\n",
    "\n",
    "# model.word2vec.Word2Vec.load(\"W2V.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}